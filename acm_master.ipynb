{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing ACM Citation Network\n",
    "#### Genevieve Peters | April 26, 2020\n",
    "#### References\n",
    "GraphX Programming Guide: ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html  \n",
    "Dataset By: Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. ArnetMiner: Extraction and Mining of Academic Social Networks. In Proceedings of the Fourteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD'2008). 990-998.  \n",
    "Dataset Description: https://aminer.org/billboard/citation  \n",
    "Spark Functions: https://spark.apache.org/docs/2.3.1/api/sql/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark configuration on yarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory=\"6000m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import spark, hadoop, and graphx packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bd-hm:8088/proxy/application_1587937229181_0026\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = yarn, app id = application_1587937229181_0026)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.conf._\n",
       "import org.apache.hadoop.io._\n",
       "import org.apache.hadoop.mapreduce.lib.input._\n",
       "import scala.util.matching.Regex\n",
       "import org.apache.spark._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.graphx._\n",
       "import org.apache.spark.rdd.RDD\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.conf._\n",
    "import org.apache.hadoop.io._\n",
    "import org.apache.hadoop.mapreduce.lib.input._\n",
    "import scala.util.matching.Regex\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore ACM Dataset\n",
    "**Task**: Load ACM dataset. Review file format.  \n",
    "**Results**: ACM dataset was filtered to include only papers that cite at least 1 other paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hadoopConf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n",
       "inputRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at filter at <console>:57\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@transient val hadoopConf = new Configuration\n",
    "hadoopConf.set(\"textinputformat.record.delimiter\",\"#*\")\n",
    "\n",
    "val inputRdd = sc.newAPIHadoopFile(\"/hadoop-user/data/citation-acm-v8.txt\",classOf[TextInputFormat],classOf[LongWritable], classOf[Text], hadoopConf)\n",
    "                 .map{case(key,value)=>value.toString}\n",
    "                 .filter(value=>value.length!=0)\n",
    "                 .filter(paper=>paper.contains(\"#%\"))//remove papers that do not reference any other paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A modification support system—automatic correction of side—effects caused by type modifications\n",
      "#@T. Tenma, Y. Sato, Y. Morimoto, M. Tanaka, T. Ichikawa\n",
      "#t1990\n",
      "#cCSC '90 Proceedings of the 1990 ACM annual conference on Cooperation\n",
      "#index5390879920f70186a0d42417\n",
      "#%5390877920f70186a0d2cae4\n",
      "#%5390878320f70186a0d329d9\n",
      "#%5390878a20f70186a0d36e5e\n",
      "#%5390878a20f70186a0d37f0a\n",
      "#%53908bad20f70186a0dc2ff5\n",
      "#!Programmers modify software for many reasons. When a part of a software is modified, side-effects of the modification are propagated to other parts of the software. By correcting these side-effects automatically, the productivity and the reliability of software development increases. This paper proposes an approach based on constraints and component-based software representation for detecting and correcting side-effects. A system based on the approach for type modifications is presented. The system is developed independent of a particular language.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sampPid: String = 5390879920f70186a0d42417\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*For Presentation*/\n",
    "val sampPid = \"5390879920f70186a0d42417\"\n",
    "inputRdd.filter(paper=>paper.contains(s\"#index$sampPid\")).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Building ACM Citation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Extract all references for each paper. Create an rdd of links.  \n",
    "**Results**:  \n",
    "- <font color=\"blue\">getLinks</font>: function that concatenates ids of all references for a paper    \n",
    "- <font color=\"blue\">citationRdd</font>(<font color=\"blue\">pid</font>,<font color=\"blue\">title</font>,<font color=\"blue\">link</font>) RDD: extracts a paper's id, title, and references  \n",
    "- <font color=\"blue\">pidRdd</font>(<font color=\"blue\">pid</font>,<font color=\"blue\">link</font>) RDD: removes the title (the title will be needed later for page rank)\n",
    "- <font color=\"blue\">indices</font>(<font color=\"blue\">index1</font>,<font color=\"blue\">index2</font>) RDD: represents every link  \n",
    "- <font color=\"blue\">linkGraph</font>(<font color=\"blue\">index1</font>,<font color=\"blue\">index2</font>) Graph: graph of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getLinks: (record: String)String\n",
       "citationRdd: org.apache.spark.rdd.RDD[(String, String, String)] = MapPartitionsRDD[7] at map at <console>:59\n",
       "pidRdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[8] at map at <console>:61\n",
       "indices: org.apache.spark.rdd.RDD[(String, Long)] = ZippedWithIndexRDD[21] at zipWithIndex at <console>:63\n",
       "linkRdd: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[29] at map at <console>:66\n",
       "linkGraph: org.apache.spark.graphx.Graph[Null,Int] = org.apache.spark.graphx.impl.GraphImpl@4587f890\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getLinks(record: String): String ={\n",
    "    val regCite = \"#%.*\".r\n",
    "    return regCite.findAllIn(record).mkString(\"\").split(\"#%\").mkString(\" \").trim()\n",
    "}\n",
    "\n",
    "val citationRdd = inputRdd.map(paper=>((paper.split(\"#index\")(1).split(\"#\")(0).trim(),paper.split(\"#\")(0).trim()),getLinks(paper)))\n",
    "                          .flatMapValues(links=>links.split(\" \"))\n",
    "                          .map{case(((pid,title),link))=>(pid,title,link)}\n",
    "\n",
    "var pidRdd = citationRdd.map{case(pid,title,link)=>(pid,link)}                             \n",
    "\n",
    "val indices = pidRdd.keys.distinct.union(pidRdd.values.distinct).distinct.zipWithIndex\n",
    "\n",
    "val linkRdd = pidRdd.join(indices).map{case(pid1,(pid2,index1))=>(pid2,index1)}\n",
    "                    .join(indices).map{case(pid2,(index1,index2))=>(index1,index2)}\n",
    "val linkGraph = Graph.fromEdgeTuples(linkRdd,null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Papers: 1369128\n",
      "Papers citing >0 papers: 916906\n",
      "Links: 8650267\n",
      "Vertices: 1357308\n",
      "Edges: 8650267\n"
     ]
    }
   ],
   "source": [
    "/*For Presentation*/\n",
    "println(\"Unique Papers: \" + indices.count)\n",
    "println(\"Papers citing >0 papers: \" + inputRdd.count())\n",
    "println(\"Links: \" + linkRdd.count())\n",
    "println(\"Vertices: \" + linkGraph.numVertices)\n",
    "println(\"Edges: \" + linkGraph.numEdges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[349088,53908bad20f70186a0dc2ff5,681896]\n",
      "[349088,5390878a20f70186a0d36e5e,1083440]\n",
      "[349088,5390878320f70186a0d329d9,881476]\n",
      "[349088,5390877920f70186a0d2cae4,1152009]\n",
      "[349088,5390878a20f70186a0d37f0a,828727]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "row: Array[org.apache.spark.sql.Row] = Array([349088])\n",
       "sampSrc: Long = 349088\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*For Presentation*/\n",
    "indices.toDF(\"pid\",\"index\").createOrReplaceTempView(\"indices\")\n",
    "linkRdd.toDF(\"src\",\"dst\").createOrReplaceTempView(\"linkRdd\")\n",
    "val row = spark.sql(\"\"\"SELECT index FROM indices WHERE (pid=='5390879920f70186a0d42417')\"\"\").take(1)\n",
    "val sampSrc = row(0).getLong(0)\n",
    "spark.sql(s\"\"\"SELECT src, pid, index FROM indices, linkRdd\n",
    "                    WHERE (src==$sampSrc) AND (dst==index)\"\"\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Visualizing the in-degree distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Compute the in-degree distribution and save rdd to file.  \n",
    "**Results**: The in-degree distribution compares the number of in-links across the dataset.  \n",
    "> <font color = \"green\" size =\"4\">&emsp;$P(k)=\\frac{n_{k}}{n}$</font>  \n",
    "Where  \n",
    "<font color = \"green\" size =\"2\">&emsp;$P(k)$</font><font size =\"2\">*- In-degree distribution*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$n_{k}$</font><font size =\"2\">*- Number of papers with k in-links*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$n$</font><font size =\"2\">*- Number of unique papers*</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inDegree: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[62] at RDD at VertexRDD.scala:57\n",
       "n: Double = 1007293.0\n",
       "pk: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[66] at map at <console>:55\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inDegree = linkGraph.inDegrees\n",
    "val n = inDegree.count().toDouble\n",
    "val pk = inDegree.map{case(dst,k)=>(k,1L)}.reduceByKey((nk1,nk2)=>(nk1+nk2))\n",
    "                 .map{case(k,nk)=>(k,(nk/n).toDouble)}\n",
    "\n",
    "pk.repartition(1).saveAsTextFile(\"/hadoop-user/data/acm_pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911\n"
     ]
    }
   ],
   "source": [
    "println(pk.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[681896,106]\n",
      "[1083440,1]\n",
      "[881476,12]\n",
      "[1152009,1149]\n",
      "[828727,46]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sampGroup: org.apache.spark.sql.DataFrame = [dst: bigint, in: int]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*For Presentation*/\n",
    "inDegree.toDF(\"dst\",\"in\").createOrReplaceTempView(\"inD\")\n",
    "val sampGroup = spark.sql(s\"SELECT lr.dst AS dst, in FROM inD i, linkRdd lr WHERE (src==$sampSrc) AND (lr.dst==i.dst)\")\n",
    "sampGroup.collect.foreach(println)\n",
    "sampGroup.createOrReplaceTempView(\"sampGroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12,0.012770862102685118]\n",
      "[1,0.3094998178285762]\n",
      "[1149,1.9855196055169647E-6]\n",
      "[106,1.4295741159722145E-4]\n",
      "[46,8.090992392481631E-4]\n"
     ]
    }
   ],
   "source": [
    "/*For Presentation*/\n",
    "pk.toDF(\"k\",\"pk\").createOrReplaceTempView(\"pk\")\n",
    "spark.sql(\"SELECT k, pk FROM pk, sampGroup WHERE (in==k)\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 Implementing the weighted page rank algorithm  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Calculate the in-weights and out-weights of each edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\" size =\"3\">&emsp;$W^{in}_{(src,dst)}=\\frac{\\#inlinks(dst)}{\\sum_{k\\in out(src)}\\#inlinks(k)}$</font>\n",
    "> Where  \n",
    "<font color = \"green\" size =\"2\">&emsp;$W_{(src,dst)}$</font><font size =\"2\">*- the in-weight of edge (src,dst)*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$\\#inlinks(dst)$</font><font size =\"2\">*- the in-degrees of dst*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;${\\sum_{k\\in out(src)}\\#inlinks(k)}$</font><font size =\"2\">*- the sum of the in-degrees of all papers that src references*</font>  \n",
    "\n",
    "<font color = \"green\" size =\"3\">&emsp;$W^{out}_{(src,dst)}=\\frac{\\#outlinks(dst)}{\\sum_{k\\in out(src)}\\#outlinks(k)}$</font>\n",
    "> Where  \n",
    "<font color = \"green\" size =\"2\">&emsp;$W_{(src,dst)}$</font><font size =\"2\">*- the out-weight of edge (src,dst)*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$\\#outlinks(dst)$</font><font size =\"2\">*- the out-degrees of dst*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;${\\sum_{k\\in out(src)}\\#outlinks(k)}$</font><font size =\"2\">*- the sum of the out-degrees of all papers that src references*</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**: \n",
    "- <font color=\"blue\">degreeGraph</font> Graph: calculates the in-degree and out-degree for every vertex\n",
    "- <font color=\"blue\">outVinK</font> Vertex, <font color=\"blue\">outVoutK</font> Vertex:     \n",
    "graph.aggregateMessages is a function to send \"messages\" to vertices in a graph. In this instance, for every src, we added the in-degrees of every paper it references and stored it as a vertex attribute. We repeated this for out-degrees. These two values represent the denominators of the in-weight and out-weight equations for each edge (src,_).\n",
    "- <font color=\"blue\">buildWeights</font> Graph: joins aggregated messages with <font color=\"blue\">degreeGraph</font> and formats its attributes\n",
    "- <font color=\"blue\">weights</font> Graph: calculates the in-weight and out-weight for every edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "degreeGraph: org.apache.spark.graphx.Graph[(Int, Int),Int] = org.apache.spark.graphx.impl.GraphImpl@59977e13\n",
       "outVinK: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[127] at RDD at VertexRDD.scala:57\n",
       "outVoutK: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[131] at RDD at VertexRDD.scala:57\n",
       "buildWeights: org.apache.spark.graphx.Graph[(Int, Int, Double, Double),Int] = org.apache.spark.graphx.impl.GraphImpl@afa5ff8\n",
       "weights: org.apache.spark.graphx.Graph[(Int, Int, Double, Double),(Double, Double)] = org.apache.spark.graphx.impl.GraphImpl@4b936383\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val degreeGraph = linkGraph.outerJoinVertices(linkGraph.inDegrees){case(id,empty,inDegOpt)=>(inDegOpt.getOrElse(0))}\n",
    "                      .outerJoinVertices(linkGraph.outDegrees){case(id,inDeg,outDegOpt)=>(inDeg,outDegOpt.getOrElse(0))}\n",
    "degreeGraph.persist()\n",
    "\n",
    "val outVinK = degreeGraph.aggregateMessages[Int](triplet=>triplet.sendToSrc(triplet.dstAttr._1),(a,b)=>(a+b))\n",
    "val outVoutK = degreeGraph.aggregateMessages[Int](triplet=>triplet.sendToSrc(triplet.dstAttr._2),(a,b)=>(a+b))\n",
    "\n",
    "val buildWeights = degreeGraph.outerJoinVertices(outVinK){case(id,(in,out),suminK)=>(in,out,suminK match\n",
    "                                                                                {case Some(suminK) => suminK.toDouble\n",
    "                                                                                 case None => 0 })}\n",
    "                            .outerJoinVertices(outVoutK){case(id,(in,out,suminK),sumoutK)=>(in,out,suminK,sumoutK match\n",
    "                                                                                {case Some(sumoutK) => sumoutK.toDouble\n",
    "                                                                                 case None => 0 })}\n",
    "\n",
    "val weights = buildWeights.mapTriplets(triplet=>(triplet.dstAttr._1/triplet.srcAttr._3, triplet.dstAttr._2/triplet.srcAttr._4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[681896,[106,8]]\n",
      "[1083440,[1,17]]\n",
      "[881476,[12,0]]\n",
      "[1152009,[1149,0]]\n",
      "[828727,[46,8]]\n"
     ]
    }
   ],
   "source": [
    "/*For Presentation*/\n",
    "degreeGraph.vertices.toDF(\"v\",\"attr\").createOrReplaceTempView(\"degG\")\n",
    "spark.sql(\"SELECT v, attr FROM degG, sampGroup WHERE (dst==v)\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[349088,1314]\n",
      "[349088,33]\n",
      "[349088,681896,[0.0806697108066971,0.24242424242424243]]\n",
      "[349088,828727,[0.0350076103500761,0.24242424242424243]]\n",
      "[349088,881476,[0.0091324200913242,0.0]]\n",
      "[349088,1083440,[7.6103500761035E-4,0.5151515151515151]]\n",
      "[349088,1152009,[0.8744292237442922,0.0]]\n"
     ]
    }
   ],
   "source": [
    "/*For Presentation*/\n",
    "outVinK.toDF(\"src\",\"vk\").createOrReplaceTempView(\"outVinK\")\n",
    "spark.sql(s\"SELECT * FROM outVinK WHERE src==$sampSrc\").collect.foreach(println)\n",
    "outVoutK.toDF(\"src\",\"vk\").createOrReplaceTempView(\"outVoutK\")\n",
    "spark.sql(s\"SELECT * FROM outVoutK WHERE src==$sampSrc\").collect.foreach(println)\n",
    "weights.edges.toDF(\"srcId\",\"dstId\",\"attr\")createOrReplaceTempView(\"weights\")\n",
    "spark.sql(s\"SELECT * FROM weights WHERE srcId==$sampSrc\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Calculate the page rank for all dst vertices. Compute 10 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\" size =\"3\">&emsp;$PR(dst)=\\frac{1-d}{N}+d * \\sum_{src\\in (dst)} PR(src)*W^{in}_{(src,dst)}*W^{out}_{(src,dst)}$</font>  \n",
    "Where  \n",
    "<font color = \"green\" size =\"2\">&emsp;$PR(dst)$</font><font size =\"2\">*- the page rank of dst*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$d$</font><font size =\"2\">*- constant => 0.85*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$N$</font><font size =\"2\">*- number of vertices => 1356780</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$PR(src)$</font><font size =\"2\">*- </font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$W^{in}_{(src,dst)}$</font><font size =\"2\">*- the in-weight of edge (src,dst)*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$W^{out}_{(src,dst)}$</font><font size =\"2\">*- the out-weight of edge (src,dst)*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$\\sum_{src\\in (dst)}$</font><font size =\"2\">*- the sum of the product of the page rank of src, the in-weight, and the out-weight for edge (src,dst)*</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "> <font color = \"blue\">edgeWeights</font>: dataframe of edges and their in-weight and out-weight  \n",
    "> <font color = \"blue\">vertPRs</font>: dataframe of vertices and their page ranks  \n",
    "> <font color = \"blue\">updatePRs</font>: dataframe that calculates a new page rank for all dst vertices  \n",
    "> <font color = \"blue\">leftJoin</font>: dataframe that updates the vertPRs dataframe with new page ranks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***1***\n",
      "[349088,1.1054459122768472E-7]\n",
      "[681896,1.4269153454260554E-6]\n",
      "[1083440,1.1075551618841542E-7]\n",
      "[881476,1.1051E-7]\n",
      "[1152009,1.1051E-7]\n",
      "[828727,4.404706174608952E-7]\n",
      "***2***\n",
      "[349088,1.1051522139576891E-7]\n",
      "[681896,3.9366235976030644E-7]\n",
      "[1083440,1.1054683799970573E-7]\n",
      "[881476,1.1051E-7]\n",
      "[1152009,1.1051E-7]\n",
      "[828727,2.3705712968161005E-7]\n",
      "***3***\n",
      "[349088,1.1051519355934562E-7]\n",
      "[681896,2.8232434451742944E-7]\n",
      "[1083440,1.1054682821247103E-7]\n",
      "[881476,1.1051E-7]\n",
      "[1152009,1.1051E-7]\n",
      "[828727,1.9915914507976426E-7]\n",
      "***4***\n",
      "[349088,1.1051519349265721E-7]\n",
      "[681896,2.5876804060096163E-7]\n",
      "[1083440,1.1054682820319479E-7]\n",
      "[881476,1.1051E-7]\n",
      "[1152009,1.1051E-7]\n",
      "[828727,1.6515345100400431E-7]\n",
      "***5***\n",
      "[349088,1.1051519348759664E-7]\n",
      "[681896,2.503740578140372E-7]\n",
      "[1083440,1.1054682820317257E-7]\n",
      "[881476,1.1051E-7]\n",
      "[1152009,1.1051E-7]\n",
      "[828727,1.6365417840411145E-7]\n",
      "***6***\n",
      "[349088,1.1051519348180802E-7]\n",
      "[681896,2.479202922966625E-7]\n",
      "[1083440,1.1054682820317089E-7]\n",
      "[881476,1.1051E-7]\n",
      "[1152009,1.1051E-7]\n",
      "[828727,1.6274311904857906E-7]\n",
      "***7***\n",
      "[349088,1.1051519348180136E-7]\n",
      "[681896,2.468105329516182E-7]\n",
      "[1083440,1.1054682820316895E-7]\n",
      "[881476,1.1051E-7]\n",
      "[1152009,1.1051E-7]\n",
      "[828727,1.6265429943787916E-7]\n",
      "***8***\n",
      "[349088,1.1051519348156645E-7]\n",
      "[681896,2.4654972271444576E-7]\n",
      "[1083440,1.1054682820316895E-7]\n",
      "[881476,1.1051E-7]\n",
      "[1152009,1.1051E-7]\n",
      "[828727,1.6243359924427621E-7]\n",
      "***9***\n",
      "[349088,1.1051519348156628E-7]\n",
      "[681896,2.4634542250041436E-7]\n",
      "[1083440,1.1054682820316887E-7]\n",
      "[881476,1.1051E-7]\n",
      "[1152009,1.1051E-7]\n",
      "[828727,1.6058355302944752E-7]\n",
      "***10***\n",
      "[349088,1.1051519348156628E-7]\n",
      "[681896,2.4632907954509403E-7]\n",
      "[1083440,1.1054682820316887E-7]\n",
      "[881476,1.1051E-7]\n",
      "[1152009,1.1051E-7]\n",
      "[828727,1.6057559654086266E-7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "N: Double = 1357308.0\n",
       "initPR: Double = 7.367524541224247E-7\n",
       "d: Double = 0.85\n",
       "edgeWeights: org.apache.spark.sql.DataFrame = [src: bigint, dst: bigint ... 2 more fields]\n",
       "vertPRs: org.apache.spark.sql.DataFrame = [v: bigint, pr: decimal(22,22)]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val N = weights.numVertices.toDouble\n",
    "val initPR = (1.0/N)\n",
    "val d = 0.85\n",
    "\n",
    "weights.edges.toDF(\"srcId\",\"dstId\",\"attr\").createOrReplaceTempView(\"weights\")\n",
    "val edgeWeights = spark.sql(\"SELECT srcId AS src, dstId AS dst, attr._1 AS win, attr._2 AS wout FROM weights\")\n",
    "edgeWeights.createOrReplaceTempView(\"eWeights\")\n",
    "\n",
    "linkGraph.vertices.map{case(v,n)=>v}.toDF(\"v\").createOrReplaceTempView(\"verts\")\n",
    "val vertPRs = spark.sql(s\"SELECT v, $initPR AS pr FROM verts\")\n",
    "vertPRs.createOrReplaceTempView(\"vPRs\")\n",
    "\n",
    "for (i<-1 to 10)\n",
    "{\n",
    "    val updatePRs = spark.sql(s\"\"\"SELECT e.dst AS dst, nanvl(((1-$d)/$N)+$d*SUM(vp.pr*e.win*e.wout), (1-$d)/$N) AS pr\n",
    "                FROM eWeights e, vPRs vp\n",
    "                WHERE e.src==vp.v\n",
    "                GROUP BY dst\"\"\")\n",
    "    updatePRs.createOrReplaceTempView(\"updatePRs\")                          \n",
    "                              \n",
    "    val leftJoin = spark.sql(s\"\"\"SELECT vp.v AS v, CASE WHEN up.pr IS NULL THEN vp.pr ELSE up.pr END AS pr\n",
    "                                    FROM vPRs vp, updatePRs up\n",
    "                                    WHERE vp.v==up.dst\"\"\")\n",
    "    leftJoin.createOrReplaceTempView(\"vPRs\")\n",
    "     /*For Presentation*/\n",
    "    println(s\"***$i***\")\n",
    "    spark.sql(s\"SELECT * FROM vPRs WHERE (v==$sampSrc)\").take(1).foreach(println)\n",
    "    spark.sql(s\"SELECT g.dst, vp.pr FROM vPRs vp, sampGroup g WHERE (vp.v==g.dst)\").collect.foreach(println)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: After the 10th iteration, find the top 10 papers with the highest ranks. Print results to a tab delimited text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**: Selected title, in-degree, and page rank from dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ranked: org.apache.spark.sql.DataFrame = [id: bigint, title: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citationRdd.map{case(pid,title,link)=>(pid,title)}.distinct.toDF(\"pid\",\"title\").createOrReplaceTempView(\"titles\")\n",
    "indices.toDF(\"pid\",\"id\").createOrReplaceTempView(\"indices\")\n",
    "inDegree.toDF(\"id\",\"in\").createOrReplaceTempView(\"inDeg\")\n",
    "\n",
    "spark.sql(\"SELECT i.id AS id, t.title AS title, n.in AS in FROM titles t, indices i, inDeg n WHERE t.pid==i.pid AND i.id==n.id\").createOrReplaceTempView(\"info\")\n",
    "\n",
    "val ranked = spark.sql(\"\"\"SELECT vp.v AS id, f.title AS title, f.in AS indegree, vp.pr AS rank\n",
    "                            FROM info f, vPRs vp\n",
    "                            WHERE f.id==vp.v\n",
    "                            ORDER BY vp.pr DESC\n",
    "                            LIMIT 10\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked.write.format(\"csv\").option(\"sep\",\"\\t\").save(\"/hadoop-user/data/acm_ranked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3 Finding the average clustering coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\" size =\"3\">&emsp;$CC(src)=\\frac{2t_{src}}{k_{src} (k_{src} -1)}$</font>  \n",
    "Where  \n",
    "<font color = \"green\" size =\"2\">&emsp;$CC(src)$</font><font size =\"2\">*- clustering coefficient of src*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$t_{src}$</font><font size =\"2\">*- number of triangles that contain src*</font>  \n",
    "<font color = \"green\" size =\"2\">&emsp;$k_{src}$</font><font size =\"2\">*- degrees of src*</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**: The number of triangles is easily computed using the graph.triangleCount() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertTri: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[4778] at RDD at VertexRDD.scala:57\n",
       "vertDeg: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[4784] at RDD at VertexRDD.scala:57\n",
       "ccv: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, Double)] = MapPartitionsRDD[4789] at map at <console>:56\n",
       "n: Long = 1156468\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertTri = linkGraph.partitionBy(PartitionStrategy.RandomVertexCut).triangleCount().vertices\n",
    "val vertDeg = linkGraph.degrees\n",
    "val ccv = vertDeg.join(vertTri).filter{case(v,(deg,tri))=>deg>1.0}.map{case(v,(deg,tri))=>(v,(2*tri)/(deg*(deg-1.0)))}\n",
    "ccv.repartition(1).saveAsTextFile(\"/hadoop-user/data/acm_ccv\")\n",
    "\n",
    "val n = ccv.count\n",
    "ccv.toDF(\"v\",\"cc\").createOrReplaceTempView(\"ccv\")\n",
    "spark.sql(s\"SELECT SUM(cc)/$n FROM ccv\").write.csv(\"/hadoop-user/data/acm_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[681896,114]\n",
      "[1083440,18]\n",
      "[881476,12]\n",
      "[1152009,1149]\n",
      "[828727,54]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "degrees: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[4784] at RDD at VertexRDD.scala:57\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val degrees = linkGraph.degrees\n",
    "degrees.toDF(\"v\",\"deg\").createOrReplaceTempView(\"deg\")\n",
    "spark.sql(s\"SELECT s.dst, d.deg FROM sampGroup s, deg d WHERE (d.v==s.dst)\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*triangles*\n",
      "[681896,165]\n",
      "[1083440,20]\n",
      "[881476,5]\n",
      "[1152009,2570]\n",
      "[828727,34]\n"
     ]
    }
   ],
   "source": [
    "vertTri.toDF(\"v\",\"t\").createOrReplaceTempView(\"vertTri\")\n",
    "println(\"*triangles*\")\n",
    "spark.sql(s\"SELECT s.dst, vt.t FROM vertTri vt, sampGroup s WHERE (vt.v==s.dst)\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[349088,0.13333333333333333]\n",
      "[681896,0.025617140195621797]\n",
      "[1083440,0.13071895424836602]\n",
      "[881476,0.07575757575757576]\n",
      "[1152009,0.003896737960292695]\n",
      "[828727,0.023759608665269043]\n"
     ]
    }
   ],
   "source": [
    "/*For Presentation*/\n",
    "spark.sql(s\"SELECT * FROM ccv WHERE (v==$sampSrc)\").take(1).foreach(println)\n",
    "spark.sql(s\"SELECT v, cc FROM ccv, sampGroup WHERE (v==dst)\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
